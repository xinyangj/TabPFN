{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene Regulatory Network (GRN) Inference with TabPFN\n",
    "\n",
    "This notebook demonstrates how to use **TabPFN** for Gene Regulatory Network inference. TabPFN is a foundation model for tabular data that can be adapted for GRN analysis by:\n",
    "\n",
    "1. **Predicting target gene expression** from transcription factors (TFs)\n",
    "2. **Extracting attention weights** to discover regulatory relationships\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **No fine-tuning required**: Uses TabPFN as a frozen foundation model with in-context learning\n",
    "- **Dual attention mechanism**: Captures both TF-TF relationships and sample-specific patterns\n",
    "- **Standard evaluation metrics**: AUROC, AUPR, Precision@k (matching DREAM challenge standards)\n",
    "- **Comprehensive visualization**: Network plots, attention heatmaps, PR/ROC curves\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, ensure you have the required packages installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installation\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Import TabPFN GRN module\n",
    "from tabpfn.grn import (\n",
    "    DREAMChallengeLoader,\n",
    "    GRNPreprocessor,\n",
    "    TabPFNGRNRegressor,\n",
    "    evaluate_grn,\n",
    "    GRNNetworkVisualizer,\n",
    "    EdgeScoreVisualizer,\n",
    "    AttentionHeatmapVisualizer,\n",
    "    create_evaluation_summary_plot,\n",
    ")\n",
    "\n",
    "print(\"\\nAll imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load DREAM Challenge Dataset\n",
    "\n",
    "We'll use the **DREAM4** dataset - a synthetic benchmark with known ground truth:\n",
    "- 10 genes network (small, for quick testing)\n",
    "- 5 TFs, 5 target genes\n",
    "- 100 samples\n",
    "- Known gold standard edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DREAM4 dataset\n",
    "data_path = Path(tempfile.gettempdir()) / \"dream_data\"\n",
    "loader = DREAMChallengeLoader(data_path=str(data_path))\n",
    "\n",
    "expression, gene_names, tf_names, gold_standard = loader.load_dream4(\n",
    "    network_size=10,  # Small network for demo\n",
    "    network_id=1\n",
    ")\n",
    "\n",
    "n_samples, n_genes = expression.shape\n",
    "n_tfs = len(tf_names)\n",
    "n_targets = n_genes - n_tfs\n",
    "\n",
    "print(f\"Dataset loaded:\")\n",
    "print(f\"  - Samples: {n_samples}\")\n",
    "print(f\"  - Total genes: {n_genes}\")\n",
    "print(f\"  - Transcription Factors (TFs): {n_tfs}\")\n",
    "print(f\"  - Target genes: {n_targets}\")\n",
    "print(f\"  - Gold standard edges: {len(gold_standard)}\")\n",
    "print(f\"\\nTF names: {tf_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Expression Data\n",
    "\n",
    "Apply z-score normalization and separate TFs from targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = GRNPreprocessor(normalization=\"zscore\")\n",
    "\n",
    "# Fit and transform\n",
    "X, y, tf_indices, target_indices = preprocessor.fit_transform(\n",
    "    expression, gene_names, tf_names\n",
    ")\n",
    "\n",
    "target_genes = preprocessor.get_target_names()\n",
    "\n",
    "print(f\"Preprocessed data:\")\n",
    "print(f\"  - Input shape (TF expression): {X.shape}\")\n",
    "print(f\"  - Output shape (target expression): {y.shape}\")\n",
    "print(f\"  - Target genes: {target_genes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train TabPFN GRN Model\n",
    "\n",
    "**Important**: TabPFN is used as a **frozen foundation model**:\n",
    "- No weight updates (fine-tuning)\n",
    "- Uses in-context learning\n",
    "- One model per target gene (independent predictions)\n",
    "\n",
    "The attention mechanism captures relationships between TFs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GRN model\n",
    "grn_model = TabPFNGRNRegressor(\n",
    "    tf_names=tf_names,\n",
    "    target_genes=target_genes,\n",
    "    n_estimators=1,  # Use 1 for faster demo (use 4-8 for better results)\n",
    "    device=\"cpu\",    # Use \"cuda\" if GPU available\n",
    "    attention_aggregation=\"mean\",  # How to aggregate attention\n",
    ")\n",
    "\n",
    "# Train (in-context learning, no weight updates)\n",
    "print(\"Training TabPFN GRN model...\")\n",
    "grn_model.fit(X, y)\n",
    "\n",
    "print(f\"\\nTraining complete:\")\n",
    "print(f\"  - Trained {len(grn_model.target_models_)} target models\")\n",
    "print(f\"  - Computed {len(grn_model.edge_scores_)} edge scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Top Predicted Edges\n",
    "\n",
    "Look at the highest-scoring predicted regulatory relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get edge scores\n",
    "edge_scores = grn_model.get_edge_scores()\n",
    "\n",
    "# Sort by score\n",
    "sorted_edges = sorted(edge_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top 10\n",
    "print(\"Top 10 predicted edges:\")\n",
    "print(\"-\" * 50)\n",
    "for i, ((tf, target), score) in enumerate(sorted_edges[:10], 1):\n",
    "    is_true = \"âœ“\" if (tf, target) in gold_standard else \" \"\n",
    "    print(f\"{i:2d}. {is_true} {tf:8s} -> {target:8s}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Against Gold Standard\n",
    "\n",
    "Compute standard GRN evaluation metrics:\n",
    "- **AUPR**: Area Under Precision-Recall (most important for sparse networks)\n",
    "- **AUROC**: Area Under ROC Curve\n",
    "- **Precision@k**: Fraction of true edges in top-k predictions\n",
    "- **Recall@k**: Fraction of true edges recovered in top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "metrics = evaluate_grn(edge_scores, gold_standard, k_values=[5, 10, 20])\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric_name, metric_value in metrics.items():\n",
    "    print(f\"{metric_name:20s}: {metric_value:.4f}\")\n",
    "\n",
    "print(\"\\nKey metrics:\")\n",
    "print(f\"  AUPR:  {metrics['aupr']:.4f} (higher is better)\")\n",
    "print(f\"  AUROC: {metrics['auroc']:.4f} (higher is better)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize the Inferred Network\n",
    "\n",
    "Create a network graph showing the top predicted regulatory relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer network with top edges\n",
    "inferred_grn = grn_model.infer_grn(top_k=15)\n",
    "\n",
    "# Visualize\n",
    "network_viz = GRNNetworkVisualizer(figsize=(12, 8))\n",
    "fig = network_viz.plot_network(\n",
    "    inferred_grn,\n",
    "    layout=\"spring\",\n",
    "    tf_color=\"#FF6B6B\",\n",
    "    target_color=\"#4ECDC4\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Network: {inferred_grn.number_of_nodes()} nodes, {inferred_grn.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Edge Score Distribution\n",
    "\n",
    "Compare score distributions for true vs false edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distribution\n",
    "score_viz = EdgeScoreVisualizer(figsize=(10, 6))\n",
    "fig = score_viz.plot_score_distribution(\n",
    "    edge_scores,\n",
    "    gold_standard=gold_standard,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Precision-Recall Curve\n",
    "\n",
    "Shows precision-recall tradeoff across different score thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curve\n",
    "fig = score_viz.plot_precision_recall_curve(\n",
    "    edge_scores,\n",
    "    gold_standard,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot ROC Curve\n",
    "\n",
    "Shows true positive rate vs false positive rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fig = score_viz.plot_roc_curve(\n",
    "    edge_scores,\n",
    "    gold_standard,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation Summary\n",
    "\n",
    "Summary bar chart of all metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary plot\n",
    "fig = create_evaluation_summary_plot(metrics)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. (Optional) Visualize Attention Patterns\n",
    "\n",
    "If attention weights were extracted, visualize the attention patterns that led to edge predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if attention weights are available\n",
    "if hasattr(grn_model, 'attention_weights_') and grn_model.attention_weights_:\n",
    "    # Get attention for first target gene\n",
    "    first_target = target_genes[0]\n",
    "    \n",
    "    if first_target in grn_model.attention_weights_:\n",
    "        attention = grn_model.attention_weights_[first_target]\n",
    "        \n",
    "        # Visualize multi-layer attention\n",
    "        attn_viz = AttentionHeatmapVisualizer(figsize=(12, 4))\n",
    "        fig = attn_viz.plot_multi_layer_attention(\n",
    "            attention,\n",
    "            tf_names=tf_names,\n",
    "        )\n",
    "        plt.suptitle(f\"Attention patterns for {first_target}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No attention weights available for visualization\")\n",
    "else:\n",
    "    print(\"Attention weights not extracted. Set 'extract_attention=True' during initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps\n",
    "\n",
    "### What We Did\n",
    "1. Loaded a DREAM4 dataset with known gold standard\n",
    "2. Preprocessed expression data (z-score normalization)\n",
    "3. Trained TabPFN as a frozen foundation model (in-context learning)\n",
    "4. Extracted edge scores from attention weights\n",
    "5. Evaluated predictions against gold standard\n",
    "6. Visualized results (network, distributions, curves)\n",
    "\n",
    "### Key Results\n",
    f"- **AUPR**: {metrics.get('aupr', 0):.4f} (primary metric for sparse GRNs)\n",
    f"- **AUROC**: {metrics.get('auroc', 0):.4f}\n",
    f"- **Precision@5**: {metrics.get('precision@5', 0):.4f}\n",
    "\n",
    "### Next Steps\n",
    "- **Try larger datasets**: DREAM5 (E. coli, yeast) for real-world performance\n",
    "- **Tune hyperparameters**: `n_estimators`, `attention_aggregation`\n",
    "- **Compare with baselines**: GENIE3, GRNBoost2, correlation\n",
    "- **Biological validation**: Check novel predictions against literature\n",
    "\n",
    "### References\n",
    "- DREAM Challenge: https://dream.broadinstitute.org/\n",
    "- TabPFN Paper: [link to paper]\n",
    "- GRN Inference Methods: Marbach et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GRN Inference Tutorial Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
